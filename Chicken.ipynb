{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/christoffelk/Machine-Learning-Fix/blob/main/Chicken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EWpzbPETPqX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow import keras \n",
        "from shutil import copyfile\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlvP_BLwTZxA"
      },
      "outputs": [],
      "source": [
        "local_zip = \"/tmp/Chicken.zip\"\n",
        "zip_ref = zipfile.ZipFile(local_zip,'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nZf8XS0T5nI"
      },
      "outputs": [],
      "source": [
        "source_dir = \"/tmp/typeoffases\"\n",
        "\n",
        "if os.path.exists(source_dir):\n",
        "  shutil.rmtree(source_dir)\n",
        "else:\n",
        "  os.makedirs(source_dir)\n",
        "\n",
        "def create_train_test_dirs(source_path):\n",
        "  train_dir = os.path.join(source_path,\"training\")\n",
        "  testing_dir = os.path.join(source_path,\"testing\")\n",
        "\n",
        "  os.makedirs(train_dir)\n",
        "  os.makedirs(testing_dir)\n",
        "\n",
        "  train_salmo_dir = os.path.join(train_dir,\"salmo\")\n",
        "  train_healthy_dir = os.path.join(train_dir,\"healthy\")\n",
        "  train_cocci_dir = os.path.join(train_dir,\"cocci\")\n",
        "  train_ncd_dir = os.path.join(train_dir,\"ncd\")\n",
        "\n",
        "  os.makedirs(train_salmo_dir)\n",
        "  os.makedirs(train_healthy_dir)\n",
        "  os.makedirs(train_cocci_dir)\n",
        "  os.makedirs(train_ncd_dir)\n",
        "\n",
        "  test_salmo_dir = os.path.join(testing_dir,\"salmo\")\n",
        "  test_healthy_dir = os.path.join(testing_dir,\"healthy\")\n",
        "  test_cocci_dir =os.path.join(testing_dir,\"cocci\")\n",
        "  test_ncd_dir = os.path.join(testing_dir,\"ncd\")\n",
        "\n",
        "  os.makedirs(test_salmo_dir)\n",
        "  os.makedirs(test_healthy_dir)\n",
        "  os.makedirs(test_cocci_dir)\n",
        "  os.makedirs(test_ncd_dir)\n",
        "\n",
        "try:\n",
        "  create_train_test_dirs(source_path=source_dir)\n",
        "except FileExistsError:\n",
        "  print(\"File already exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SpFa-oVMZcy"
      },
      "outputs": [],
      "source": [
        "def split_data(SOURCE,TRAINING,TESTING,SPLIT_SIZE):\n",
        "  file_not_zero=[]\n",
        "  for files in os.listdir(SOURCE):\n",
        "    if os.path.getsize(SOURCE+files)>0:\n",
        "      file_not_zero.append(files)\n",
        "    else:\n",
        "      print(\"{} is zero length\".format(files))\n",
        "    \n",
        "    training_size = int(len(file_not_zero)* SPLIT_SIZE)\n",
        "    testing_size = int(len(file_not_zero) - training_size)\n",
        "    shuffle = random.sample(file_not_zero,len(file_not_zero))\n",
        "    training_file = shuffle[0:training_size]\n",
        "    testing_file = shuffle[-testing_size:]\n",
        "  \n",
        "  for files in training_file:\n",
        "    source = SOURCE + files\n",
        "    destination = TRAINING+files\n",
        "    copyfile(source,destination)\n",
        "\n",
        "  for files in testing_file:\n",
        "    source = SOURCE + files\n",
        "    destination = TESTING + files\n",
        "    copyfile(source,destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCJ0VdwqNbxW",
        "outputId": "8e63fa65-06bc-4da9-af57-286cb772824c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "There are 2362 images of salmo for training\n",
            "There are 2163 images of healhy for training\n",
            "There are 2228 images of cocci for training\n",
            "There are 505 images of ncd for training\n",
            "There are 263 images of salmo for testing\n",
            "There are 241 images of healthy for testing\n",
            "There are 248 images of cocci for testing\n",
            "There are 57 images of ncd for testing\n"
          ]
        }
      ],
      "source": [
        "SALMO_SOURCE_DIR = \"/tmp/chickenfasespart/salmo/\"\n",
        "HEALTHY_SOURCE_DIR = \"/tmp/chickenfasespart/healthy/\"\n",
        "COCCI_SOURCE_DIR = \"/tmp/chickenfasespart/cocci/\"\n",
        "NCD_SOURCE_DIR = \"/tmp/chickenfasespart/ncd/\"\n",
        "\n",
        "TRAINING_DIR = \"/tmp/typeoffases/training/\"\n",
        "TESTING_DIR = \"/tmp/typeoffases/testing/\"\n",
        "\n",
        "TRAINING_SALMO_DIR = os.path.join(TRAINING_DIR,\"salmo/\")\n",
        "TESTING_SALMO_DIR = os.path.join(TESTING_DIR,\"salmo/\")\n",
        "\n",
        "TRAINING_HEALTHY_DIR = os.path.join(TRAINING_DIR,\"healthy/\")\n",
        "TESTING_HEALTHY_DIR = os.path.join(TESTING_DIR,\"healthy/\")\n",
        "\n",
        "TRAINING_COCCI_DIR = os.path.join(TRAINING_DIR,\"cocci/\")\n",
        "TESTING_COCCI_DIR = os.path.join(TESTING_DIR,\"cocci/\")\n",
        "\n",
        "TRAINING_NCD_DIR = os.path.join(TRAINING_DIR,\"ncd/\")\n",
        "TESTING_NCD_DIR = os.path.join(TESTING_DIR,\"ncd/\")\n",
        "\n",
        "if len(os.listdir(TRAINING_SALMO_DIR))>0:\n",
        "  for file in os.scandir(TRAINING_SALMO_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TRAINING_HEALTHY_DIR))>0:\n",
        "  for file in os.scandir(TRAINING_HEALTHY_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TRAINING_COCCI_DIR))>0:\n",
        "  for file in os.scandir(TRAINING_COCCI_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TRAINING_NCD_DIR))>0:\n",
        "  for file in os.scandir(TRAINING_NCD_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_SALMO_DIR))>0:\n",
        "  for file in os.scandir(TESTING_SALMO_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_HEALTHY_DIR))>0:\n",
        "  for file in os.scandir(TESTING_HEALTHY_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_COCCI_DIR))>0:\n",
        "  for file in os.scandir(TESTING_COCCI_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_NCD_DIR))>0:\n",
        "  for file in os.scandir(TESTING_NCD_DIR):\n",
        "    os.remove(file.path)\n",
        "\n",
        "\n",
        "split_size = .9\n",
        "split_data(SALMO_SOURCE_DIR,TRAINING_SALMO_DIR,TESTING_SALMO_DIR,split_size)\n",
        "split_data(HEALTHY_SOURCE_DIR,TRAINING_HEALTHY_DIR,TESTING_HEALTHY_DIR,split_size)\n",
        "split_data(COCCI_SOURCE_DIR,TRAINING_COCCI_DIR,TESTING_COCCI_DIR,split_size)\n",
        "split_data(NCD_SOURCE_DIR,TRAINING_NCD_DIR,TESTING_NCD_DIR,split_size)\n",
        "\n",
        "\n",
        "print(f\"\\n\\nThere are {len(os.listdir(TRAINING_SALMO_DIR))} images of salmo for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_HEALTHY_DIR))} images of healhy for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_COCCI_DIR))} images of cocci for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_NCD_DIR))} images of ncd for training\")\n",
        "\n",
        "print(f\"There are {len(os.listdir(TESTING_SALMO_DIR))} images of salmo for testing\")\n",
        "print(f\"There are {len(os.listdir(TESTING_HEALTHY_DIR))} images of healthy for testing\")\n",
        "print(f\"There are {len(os.listdir(TESTING_COCCI_DIR))} images of cocci for testing\")\n",
        "print(f\"There are {len(os.listdir(TESTING_NCD_DIR))} images of ncd for testing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uu_kB3N1Skn2"
      },
      "outputs": [],
      "source": [
        "def train_val_generators(TRAINING_DIR,VALIDATION_DIR):\n",
        "  train_datagen = ImageDataGenerator(rescale=1.0/255.0,\n",
        "                                     rotation_range=40,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip = True,\n",
        "                                     fill_mode = \"nearest\")\n",
        "  \n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size =104,\n",
        "                                                      class_mode=\"categorical\",\n",
        "                                                      target_size=(224,224))\n",
        "  \n",
        "  validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                               batch_size=104,\n",
        "                                                               class_mode = \"categorical\",\n",
        "                                                               target_size=(224,224))\n",
        "  \n",
        "  return train_generator,validation_generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSblhMO5T9Ak",
        "outputId": "40361335-6204-4892-9b0d-39d445d1c3f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7258 images belonging to 4 classes.\n",
            "Found 809 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator,validation_generator = train_val_generators(TRAINING_DIR,TESTING_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhDYO3151FRD"
      },
      "outputs": [],
      "source": [
        "class Callback(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self,epoch,logs={}):\n",
        "    if(logs.get('loss') <= 0.09 and logs.get('val_loss') <=0.09):\n",
        "      print(\"stop the training because the accuracy is already on target\")\n",
        "      self.model.stop_training=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODxxMKZJUG1c"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu',input_shape=(224,224,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),  \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),    tf.keras.layers.MaxPooling2D(2,2),    \n",
        "    tf.keras.layers.Conv2D(256,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(512,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "     tf.keras.layers.Conv2D(512,(3,3),activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(), \n",
        "    tf.keras.layers.Dense(1024, activation='relu'), \n",
        "    tf.keras.layers.Dense(4, activation='softmax')  \n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss=\"categorical_crossentropy\",\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YVxDqnaY4f9",
        "outputId": "b85f9ffe-35da-45cc-a9c4-c32c857f908a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "70/70 [==============================] - 969s 14s/step - loss: 0.9794 - accuracy: 0.5602 - val_loss: 0.7115 - val_accuracy: 0.7120\n",
            "Epoch 2/50\n",
            "70/70 [==============================] - 959s 14s/step - loss: 0.6651 - accuracy: 0.7600 - val_loss: 0.6111 - val_accuracy: 0.7565\n",
            "Epoch 3/50\n",
            "70/70 [==============================] - 954s 14s/step - loss: 0.5549 - accuracy: 0.8002 - val_loss: 0.4774 - val_accuracy: 0.8208\n",
            "Epoch 4/50\n",
            "70/70 [==============================] - 943s 13s/step - loss: 0.5231 - accuracy: 0.8157 - val_loss: 0.4881 - val_accuracy: 0.8059\n",
            "Epoch 5/50\n",
            "70/70 [==============================] - 938s 13s/step - loss: 0.4571 - accuracy: 0.8326 - val_loss: 0.4096 - val_accuracy: 0.8443\n",
            "Epoch 6/50\n",
            "70/70 [==============================] - 937s 13s/step - loss: 0.4275 - accuracy: 0.8453 - val_loss: 0.3802 - val_accuracy: 0.8554\n",
            "Epoch 7/50\n",
            "70/70 [==============================] - 938s 13s/step - loss: 0.3917 - accuracy: 0.8582 - val_loss: 0.2970 - val_accuracy: 0.8863\n",
            "Epoch 8/50\n",
            "70/70 [==============================] - 944s 13s/step - loss: 0.3456 - accuracy: 0.8781 - val_loss: 0.2797 - val_accuracy: 0.8925\n",
            "Epoch 9/50\n",
            "70/70 [==============================] - 952s 14s/step - loss: 0.3211 - accuracy: 0.8861 - val_loss: 0.2834 - val_accuracy: 0.9023\n",
            "Epoch 10/50\n",
            "70/70 [==============================] - 935s 13s/step - loss: 0.2973 - accuracy: 0.8954 - val_loss: 0.2383 - val_accuracy: 0.9172\n",
            "Epoch 11/50\n",
            "70/70 [==============================] - 955s 14s/step - loss: 0.3127 - accuracy: 0.8968 - val_loss: 0.2547 - val_accuracy: 0.9110\n",
            "Epoch 12/50\n",
            "70/70 [==============================] - 932s 13s/step - loss: 0.3095 - accuracy: 0.8912 - val_loss: 0.2483 - val_accuracy: 0.9122\n",
            "Epoch 13/50\n",
            "70/70 [==============================] - 929s 13s/step - loss: 0.2598 - accuracy: 0.9071 - val_loss: 0.2415 - val_accuracy: 0.9110\n",
            "Epoch 14/50\n",
            "70/70 [==============================] - 930s 13s/step - loss: 0.2651 - accuracy: 0.9087 - val_loss: 0.2263 - val_accuracy: 0.9197\n",
            "Epoch 15/50\n",
            "70/70 [==============================] - 927s 13s/step - loss: 0.2448 - accuracy: 0.9138 - val_loss: 0.2524 - val_accuracy: 0.9122\n",
            "Epoch 16/50\n",
            "70/70 [==============================] - 925s 13s/step - loss: 0.2431 - accuracy: 0.9142 - val_loss: 0.1791 - val_accuracy: 0.9345\n",
            "Epoch 17/50\n",
            "70/70 [==============================] - 930s 13s/step - loss: 0.2229 - accuracy: 0.9182 - val_loss: 0.1997 - val_accuracy: 0.9258\n",
            "Epoch 18/50\n",
            "70/70 [==============================] - 932s 13s/step - loss: 0.2236 - accuracy: 0.9193 - val_loss: 0.2575 - val_accuracy: 0.8974\n",
            "Epoch 19/50\n",
            "70/70 [==============================] - 929s 13s/step - loss: 0.2199 - accuracy: 0.9202 - val_loss: 0.2155 - val_accuracy: 0.9234\n",
            "Epoch 20/50\n",
            "70/70 [==============================] - 926s 13s/step - loss: 0.2102 - accuracy: 0.9270 - val_loss: 0.2190 - val_accuracy: 0.9221\n",
            "Epoch 21/50\n",
            "70/70 [==============================] - 929s 13s/step - loss: 0.2141 - accuracy: 0.9222 - val_loss: 0.1702 - val_accuracy: 0.9370\n",
            "Epoch 22/50\n",
            "70/70 [==============================] - 932s 13s/step - loss: 0.2032 - accuracy: 0.9286 - val_loss: 0.1755 - val_accuracy: 0.9382\n",
            "Epoch 23/50\n",
            "70/70 [==============================] - 927s 13s/step - loss: 0.1936 - accuracy: 0.9314 - val_loss: 0.1760 - val_accuracy: 0.9357\n",
            "Epoch 24/50\n",
            "70/70 [==============================] - 930s 13s/step - loss: 0.2102 - accuracy: 0.9264 - val_loss: 0.2016 - val_accuracy: 0.9283\n",
            "Epoch 25/50\n",
            "70/70 [==============================] - 927s 13s/step - loss: 0.1955 - accuracy: 0.9273 - val_loss: 0.1852 - val_accuracy: 0.9382\n",
            "Epoch 26/50\n",
            "70/70 [==============================] - 932s 13s/step - loss: 0.1933 - accuracy: 0.9363 - val_loss: 0.1630 - val_accuracy: 0.9394\n",
            "Epoch 27/50\n",
            "70/70 [==============================] - 926s 13s/step - loss: 0.1797 - accuracy: 0.9394 - val_loss: 0.1829 - val_accuracy: 0.9308\n",
            "Epoch 28/50\n",
            "70/70 [==============================] - 919s 13s/step - loss: 0.1608 - accuracy: 0.9414 - val_loss: 0.1632 - val_accuracy: 0.9468\n",
            "Epoch 29/50\n",
            "70/70 [==============================] - 919s 13s/step - loss: 0.1746 - accuracy: 0.9398 - val_loss: 0.1610 - val_accuracy: 0.9481\n",
            "Epoch 30/50\n",
            "70/70 [==============================] - 923s 13s/step - loss: 0.1750 - accuracy: 0.9381 - val_loss: 0.1399 - val_accuracy: 0.9567\n",
            "Epoch 31/50\n",
            "70/70 [==============================] - 921s 13s/step - loss: 0.1601 - accuracy: 0.9456 - val_loss: 0.1740 - val_accuracy: 0.9394\n",
            "Epoch 32/50\n",
            "70/70 [==============================] - 918s 13s/step - loss: 0.1481 - accuracy: 0.9490 - val_loss: 0.1609 - val_accuracy: 0.9419\n",
            "Epoch 33/50\n",
            "70/70 [==============================] - 924s 13s/step - loss: 0.1494 - accuracy: 0.9481 - val_loss: 0.1448 - val_accuracy: 0.9493\n",
            "Epoch 34/50\n",
            "70/70 [==============================] - 923s 13s/step - loss: 0.1400 - accuracy: 0.9504 - val_loss: 0.1550 - val_accuracy: 0.9419\n",
            "Epoch 35/50\n",
            "70/70 [==============================] - 920s 13s/step - loss: 0.1418 - accuracy: 0.9476 - val_loss: 0.1624 - val_accuracy: 0.9419\n",
            "Epoch 36/50\n",
            "70/70 [==============================] - 919s 13s/step - loss: 0.1683 - accuracy: 0.9394 - val_loss: 0.1966 - val_accuracy: 0.9283\n",
            "Epoch 37/50\n",
            "70/70 [==============================] - 916s 13s/step - loss: 0.1379 - accuracy: 0.9504 - val_loss: 0.1834 - val_accuracy: 0.9357\n",
            "Epoch 38/50\n",
            "70/70 [==============================] - 922s 13s/step - loss: 0.1348 - accuracy: 0.9515 - val_loss: 0.1794 - val_accuracy: 0.9357\n",
            "Epoch 39/50\n",
            "70/70 [==============================] - 921s 13s/step - loss: 0.1457 - accuracy: 0.9471 - val_loss: 0.2049 - val_accuracy: 0.9370\n",
            "Epoch 40/50\n",
            "66/70 [===========================>..] - ETA: 50s - loss: 0.1334 - accuracy: 0.9532 "
          ]
        }
      ],
      "source": [
        "model = create_model()\n",
        "callbacks = Callback()\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=50,\n",
        "                    verbose=1,\n",
        "                    callbacks=[callbacks],\n",
        "                    validation_data=validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4S9kSL3yrva"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u0XtoK7y1Ss"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image\n",
        "\n",
        "images = os.listdir(\"/tmp/Testing and Prediction\")\n",
        "\n",
        "print(images)\n",
        "\n",
        "for i in images:\n",
        "    print()\n",
        "#     # predicting images\n",
        "    path = '/tmp/Testing and Prediction/' + i\n",
        "    img = image.load_img(path, target_size=(224, 224))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    \n",
        "    images = np.vstack([x])\n",
        "    classes = model.predict(images, batch_size=10)\n",
        "    print(path)\n",
        "    print(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvzTMogDNaYe",
        "outputId": "9be72ae2-3b4e-4497-f333-e4287be1d0f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: tmp/modelexport/assets\n"
          ]
        }
      ],
      "source": [
        "export_dir = 'tmp/modelexport'\n",
        "tf.saved_model.save(model,export_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2LbXfvjNr4_"
      },
      "outputs": [],
      "source": [
        "mode = \"Speed\"\n",
        "if mode == 'Storage' :\n",
        "  optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "else:\n",
        "  optimization = tf.lite.Optimize.DEFAULT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkW-6tXpOFP_",
        "outputId": "fff50d82-341a-4267-c0c8-b37fb56c9df1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Optimization option OPTIMIZE_FOR_LATENCY is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_LATENCY is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_LATENCY is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "converter.optimizations = [optimization]\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGyt9nAmOTl0",
        "outputId": "7316f0a6-b69d-497b-8d7a-3c502cbc18e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19284688"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tflite_model_file = pathlib.Path('/model.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of chicken",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}